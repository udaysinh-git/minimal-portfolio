<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
  <channel>
    <title></title>
    <link></link>
    <description></description>
    <language>en-us</language>
    <lastBuildDate></lastBuildDate>
    
    <item>
      <title>Creating a Real-Time Hand-Tracking Drawing Application with Python</title>
      <link>/posts/Finger-Funk/</link>
      <description><![CDATA[<h2>Introduction</h2>
<p>Welcome to <strong>Finger-Funk</strong>! In this blog post, we will build a real-time hand-tracking drawing application using Python, OpenCV, and MediaPipe. This application allows users to draw on the screen using hand gestures, change colors, adjust brush thickness, and erase drawings‚Äîall without touching the screen. By the end of this tutorial, you'll have a functional drawing tool that leverages computer vision for an interactive experience.</p>
<h3>üé• Demo</h3>
<p>Check out this demo of <strong>Finger-Funk</strong> in action:</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/PZiuoMNmJgk" style="position: absolute; top:0; left: 0; width: 100%; height: 100%;" frameborder="0" allowfullscreen></iframe>
</div>
<h3>üìÇ GitHub Repository</h3>
<p>You can find the complete source code for <strong>Finger-Funk</strong> on GitHub. Click the image below to visit the repository and get started:</p>
<p><a href="https://github.com/udaysinh-git/Finger-Funk"><img src="https://img.shields.io/badge/GitHub-Finger--Funk-blue?logo=github" alt="Finger-Funk GitHub Repository"></a></p>
<h2>Finger-Funk: Your Hands, Your Canvas</h2>
<p><strong>Finger-Funk</strong> is a virtual canvas application that captures hand movements via your webcam to create digital art. Powered by OpenCV and MediaPipe, it transforms simple gestures into dynamic drawing actions.</p>
<h3>üé®üñêÔ∏è Draw with Your Hands: A Gesture-Based Drawing App</h3>
<p>This fun and interactive application allows you to draw on your screen using hand gestures detected by your webcam. Whether you're changing colors, adjusting brush thickness, or erasing drawings, Finger-Funk makes digital art intuitive and hands-free.</p>
<h2>Features</h2>
<ul>
<li><strong>Hand Gesture Detection</strong>: Uses MediaPipe to detect hand landmarks.</li>
<li><strong>Drawing with Fingers</strong>: Draw lines by bringing your thumb and index finger close together.</li>
<li><strong>Color Selection</strong>: Change drawing colors by hovering over color boxes.</li>
<li><strong>Dynamic Thickness</strong>: Adjust line thickness based on the number of fingers touching the thumb.</li>
<li><strong>Erase Mode</strong>: Erase parts of your drawing by spreading your thumb and pinky finger apart.</li>
<li><strong>Clear Screen</strong>: Clear the entire drawing by raising the middle finger on both hands.</li>
</ul>
<h2>Prerequisites</h2>
<p>Before we begin, ensure you have the following installed on your Windows machine:</p>
<ul>
<li>Python 3.x</li>
<li>OpenCV</li>
<li>MediaPipe</li>
</ul>
<p>You can install the required libraries using pip:</p>
<pre><code class="language-bash">pip install opencv-python mediapipe
</code></pre>
<h2>Installation</h2>
<p>Clone the repository:</p>
<pre><code class="language-bash">git clone https://github.com/udaysinh-git/finger-funk.git
cd finger-funk
</code></pre>
<p>Create a virtual environment and activate it:</p>
<pre><code class="language-bash">python -m venv .venv
.venv\Scripts\activate  # On Windows use `.venv\Scripts\activate`
</code></pre>
<p>Install the required packages:</p>
<pre><code class="language-bash">pip install -r requirements.txt
</code></pre>
<h2>Usage</h2>
<p>Run the Jupyter Notebook:</p>
<pre><code class="language-bash">jupyter notebook main.ipynb
</code></pre>
<p>Follow the instructions in the notebook to start the application.</p>
<h2>Step-by-Step Guide</h2>
<h3>Step 1: Import Libraries</h3>
<p>First, let's import the necessary libraries for our application.</p>
<pre><code class="language-python">import cv2
import mediapipe as mp
import time
</code></pre>
<h3>Step 2: Initialize MediaPipe and OpenCV</h3>
<p>We'll initialize MediaPipe for hand tracking and set up OpenCV for video capture.</p>
<pre><code class="language-python"># Initialize MediaPipe Hands
mp_hands = mp.solutions.hands
hands = mp_hands.Hands()
mp_drawing = mp.solutions.drawing_utils

# Initialize OpenCV
cap = cv2.VideoCapture(0)
</code></pre>
<h3>Step 3: Set Up Drawing Settings</h3>
<p>Define the initial settings for drawing, including colors, thickness, and other configurations.</p>
<pre><code class="language-python"># Drawing settings
draw_color = (0, 255, 0)  # Green color for drawing
draw_thickness = 5
erase_thickness = 50  # Thickness of the eraser
drawing_points = []  # List to store drawing points

# Color options
colors = [(0, 255, 0), (0, 0, 255), (255, 0, 0), (255, 255, 0)]
color_index = 0

# Color selection box settings
color_box_size = 50
color_box_positions = [(10, 10), (70, 10), (130, 10), (190, 10)]

# Time tracking for clearing delay
clear_time = None
clear_delay = 5 

# Track previous points for drawing lines
prev_point = None
</code></pre>
<h3>Step 4: Define Helper Functions</h3>
<p>We'll create a function to determine if the middle finger is up, which will be used to clear the drawings.</p>
<pre><code class="language-python">def is_middle_finger_up(thumb_tip_coords, index_tip_coords, middle_tip_coords, ring_tip_coords, pinky_tip_coords):
    # Check if middle finger is up and other fingers are down
    return (middle_tip_coords[1] &lt; index_tip_coords[1] and
            middle_tip_coords[1] &lt; ring_tip_coords[1] and
            middle_tip_coords[1] &lt; thumb_tip_coords[1] and
            middle_tip_coords[1] &lt; pinky_tip_coords[1] and
            index_tip_coords[1] &gt; thumb_tip_coords[1] and
            ring_tip_coords[1] &gt; thumb_tip_coords[1] and
            pinky_tip_coords[1] &gt; thumb_tip_coords[1])
</code></pre>
<h3>Step 5: Create a Resizable Window</h3>
<p>Set up a named window that can be resized for better visibility.</p>
<pre><code class="language-python"># Create a named window with the ability to resize
cv2.namedWindow('Hand Drawing App', cv2.WINDOW_NORMAL)
# Set the desired window size
cv2.resizeWindow('Hand Drawing App', 1366, 768)
</code></pre>
<h3>Step 6: Implement the Main Loop</h3>
<p>The core of the application captures video frames, processes hand landmarks, and handles drawing based on user gestures.</p>
<pre><code class="language-python">while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Flip the frame horizontally for a selfie-view display
    frame = cv2.flip(frame, 1)
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # Draw color selection boxes
    for i, color in enumerate(colors):
        cv2.rectangle(frame, color_box_positions[i], 
                      (color_box_positions[i][0] + color_box_size, color_box_positions[i][1] + color_box_size), 
                      color, -1)

    # Process the frame and detect hands
    results = hands.process(frame_rgb)

    right_hand_middle_up = False
    left_hand_middle_up = False

    if results.multi_hand_landmarks:
        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
            # Draw hand landmarks
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # Get hand label (Left or Right)
            hand_label = handedness.classification[0].label

            # Get coordinates of finger tips
            thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]
            index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]
            middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]
            ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]
            pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]

            # Convert normalized coordinates to pixel coordinates
            h, w, _ = frame.shape
            thumb_tip_coords = (int(thumb_tip.x * w), int(thumb_tip.y * h))
            index_tip_coords = (int(index_tip.x * w), int(index_tip.y * h))
            middle_tip_coords = (int(middle_tip.x * w), int(middle_tip.y * h))
            ring_tip_coords = (int(ring_tip.x * w), int(ring_tip.y * h))
            pinky_tip_coords = (int(pinky_tip.x * w), int(pinky_tip.y * h))

            if hand_label == 'Right':
                # Check if thumb and index finger are close to each other
                distance_thumb_index = ((thumb_tip_coords[0] - index_tip_coords[0]) ** 2 + 
                                        (thumb_tip_coords[1] - index_tip_coords[1]) ** 2) ** 0.5
                if distance_thumb_index &lt; 30 and (clear_time is None or time.time() - clear_time &gt; clear_delay):
                    if prev_point is not None:
                        drawing_points.append((prev_point, index_tip_coords, draw_color, draw_thickness))
                    prev_point = index_tip_coords
                else:
                    prev_point = None

                # Check if index finger is over a color box
                for i, (x, y) in enumerate(color_box_positions):
                    if x &lt; index_tip_coords[0] &lt; x + color_box_size and y &lt; index_tip_coords[1] &lt; y + color_box_size:
                        draw_color = colors[i]

                # Adjust thickness based on the number of fingers touching the thumb
                touching_fingers = 0
                for tip_coords in [index_tip_coords, middle_tip_coords, ring_tip_coords, pinky_tip_coords]:
                    distance = ((thumb_tip_coords[0] - tip_coords[0]) ** 2 + 
                                (thumb_tip_coords[1] - tip_coords[1]) ** 2) ** 0.5
                    if distance &lt; 30:
                        touching_fingers += 1

                draw_thickness = 5 + touching_fingers * 5

                # Check if middle finger is up
                if is_middle_finger_up(thumb_tip_coords, index_tip_coords, middle_tip_coords, ring_tip_coords, pinky_tip_coords):
                    right_hand_middle_up = True

            elif hand_label == 'Left':
                # Check if hand is wide open (thumb and pinky are far apart)
                distance_thumb_pinky = ((thumb_tip_coords[0] - pinky_tip_coords[0]) ** 2 + 
                                        (thumb_tip_coords[1] - pinky_tip_coords[1]) ** 2) ** 0.5
                if distance_thumb_pinky &gt; 100:
                    # Erase points near the hand
                    margin = 10
                    drawing_points = [point for point in drawing_points if 
                                      ((point[0][0] - index_tip_coords[0]) ** 2 + 
                                       (point[0][1] - index_tip_coords[1]) ** 2) ** 0.5 &gt; erase_thickness or
                                      point[0][0] &lt; margin or point[0][0] &gt; w - margin or 
                                      point[0][1] &lt; margin or point[0][1] &gt; h - margin]

                # Check if middle finger is up
                if is_middle_finger_up(thumb_tip_coords, index_tip_coords, middle_tip_coords, ring_tip_coords, pinky_tip_coords):
                    left_hand_middle_up = True

    # Clear all drawings if middle finger is up on both hands
    if right_hand_middle_up and left_hand_middle_up:
        drawing_points.clear()
        clear_time = time.time()  # Record the time when the drawing is cleared

    # Draw all lines in drawing_points
    for start_point, end_point, color, thickness in drawing_points:
        cv2.line(frame, start_point, end_point, color, thickness)

    cv2.imshow('Hand Drawing App', frame)

    if cv2.waitKey(1) &amp; 0xFF == 27:
        break
</code></pre>
<h3>Step 7: Release Resources</h3>
<p>After exiting the main loop, release the video capture and close all OpenCV windows.</p>
<pre><code class="language-python">cap.release()
cv2.destroyAllWindows()
</code></pre>
<h2>How It Works</h2>
<ul>
<li><strong>Hand Detection</strong>: The application uses MediaPipe to detect hand landmarks in real-time.</li>
<li><strong>Drawing</strong>: When the thumb and index finger are close together, the application starts drawing lines.</li>
<li><strong>Color Selection</strong>: Hover over the color boxes at the top left corner to change the drawing color.</li>
<li><strong>Erase Mode</strong>: Spread your thumb and pinky finger apart to erase parts of the drawing.</li>
<li><strong>Clear Screen</strong>: Raise the middle finger on both hands to clear the entire screen.</li>
</ul>
<h2>Dependencies</h2>
<ul>
<li>OpenCV</li>
<li>MediaPipe</li>
<li>NumPy</li>
</ul>
<h2>Contributing</h2>
<p>Feel free to fork this repository and contribute by submitting pull requests. Any improvements or new features are welcome!</p>
<h2>License</h2>
<p>This project is licensed under the MIT License. See the <a href="https://github.com/udaysinh-git/Finger-Funk/blob/main/LICENSE">LICENSE</a> file for details.</p>
<h2>References</h2>
<ul>
<li><a href="https://google.github.io/mediapipe/solutions/hands.html">MediaPipe Hands Documentation</a></li>
<li><a href="https://opencv.org/">OpenCV Documentation</a></li>
<li><a href="https://github.com/google/mediapipe">MediaPipe GitHub Repository</a></li>
<li><a href="https://github.com/opencv/opencv">OpenCV GitHub Repository</a></li>
</ul>
<h2>Acknowledgements</h2>
<ul>
<li>OpenCV</li>
<li>MediaPipe</li>
</ul>
<p>Enjoy drawing with your hands! üé®üñêÔ∏è</p>
]]></description>
      <pubDate></pubDate>
      <guid>/posts/Finger-Funk/</guid>
    </item>
    
    <item>
      <title>Building a Simple Cat vs Dog Classifier Using Python</title>
      <link>/posts/cats_vs_dogs/</link>
      <description><![CDATA[<h2>Introduction</h2>
<p>In this blog post, we will walk through the process of creating a simple artificial intelligence model that can differentiate between images of cats and dogs. We will use Python and some popular libraries to achieve this. By the end of this tutorial, you will have a basic understanding of how to build and train a machine learning model for image classification.</p>
<h2>Prerequisites</h2>
<p>Before we start, make sure you have the following installed:</p>
<ul>
<li>Python 3.x</li>
<li>TensorFlow</li>
<li>Keras</li>
<li>NumPy</li>
<li>Matplotlib</li>
<li>Requests</li>
</ul>
<p>You can install these libraries using pip:</p>
<pre><code class="language-bash">pip install tensorflow keras numpy matplotlib requests
</code></pre>
<h2>Step 1: Import Libraries</h2>
<p>First, let's import the necessary libraries.</p>
<pre><code class="language-python">import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
import numpy as np
import matplotlib.pyplot as plt
import requests
from PIL import Image
from io import BytesIO
</code></pre>
<h2>Step 2: Load and Preprocess Data</h2>
<p>For this tutorial, we will use images from two APIs: <a href="https://api.thecatapi.com/v1/images/search">The Cat API</a> for cat images and <a href="https://dog.ceo/api/breeds/image/random">Dog CEO's Dog API</a> for dog images.</p>
<h3>Fetching Cat Images</h3>
<pre><code class="language-python">cat_url = &quot;https://api.thecatapi.com/v1/images/search&quot;
response = requests.get(cat_url)
cat_data = response.json()
cat_image_url = cat_data[0]['url']

response = requests.get(cat_image_url)
cat_img = Image.open(BytesIO(response.content))
cat_img = cat_img.resize((128, 128))
cat_img = np.array(cat_img)
</code></pre>
<h3>Fetching Dog Images</h3>
<pre><code class="language-python">dog_url = &quot;https://dog.ceo/api/breeds/image/random&quot;
response = requests.get(dog_url)
dog_data = response.json()
dog_image_url = dog_data['message']

response = requests.get(dog_image_url)
dog_img = Image.open(BytesIO(response.content))
dog_img = dog_img.resize((128, 128))
dog_img = np.array(dog_img)
</code></pre>
<h3>Displaying the Images</h3>
<pre><code class="language-python">plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.title(&quot;Cat&quot;)
plt.imshow(cat_img)

plt.subplot(1, 2, 2)
plt.title(&quot;Dog&quot;)
plt.imshow(dog_img)

plt.show()
</code></pre>
<p><img src="https://cdn2.thecatapi.com/images/MTY5OTE4Nw.jpg" alt="Cat Image">
<img src="https://images.dog.ceo/breeds/hound-afghan/n02088094_1003.jpg" alt="Dog Image"></p>
<h2>Step 3: Build the Model</h2>
<p>We will use a simple Convolutional Neural Network (CNN) for our classifier.</p>
<pre><code class="language-python">model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(2, activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
</code></pre>
<h2>Step 4: Train the Model</h2>
<p>For simplicity, we will use the same images for training. In a real-world scenario, you should use a larger dataset.</p>
<pre><code class="language-python">X = np.array([cat_img, dog_img])
y = np.array([0, 1])  # 0 for cat, 1 for dog

model.fit(X, y, epochs=10)
</code></pre>
<h2>Step 5: Evaluate the Model</h2>
<p>Let's test our model with a new image.</p>
<pre><code class="language-python">test_url = &quot;https://api.thecatapi.com/v1/images/search&quot;
response = requests.get(test_url)
test_data = response.json()
test_image_url = test_data[0]['url']

response = requests.get(test_image_url)
test_img = Image.open(BytesIO(response.content))
test_img = test_img.resize((128, 128))
test_img = np.array(test_img)

prediction = model.predict(np.expand_dims(test_img, axis=0))
predicted_class = np.argmax(prediction)

if predicted_class == 0:
    print(&quot;It's a cat!&quot;)
else:
    print(&quot;It's a dog!&quot;)
</code></pre>
<h2>Conclusion</h2>
<p>In this tutorial, we built a simple image classifier that can differentiate between cats and dogs using Python and TensorFlow. While this is a basic example, it provides a foundation for more complex image classification tasks. Happy coding!</p>
<h2>References</h2>
<ul>
<li><a href="https://api.thecatapi.com/v1/images/search">The Cat API</a></li>
<li><a href="https://dog.ceo/api/breeds/image/random">Dog CEO's Dog API</a></li>
<li><a href="https://www.tensorflow.org/">TensorFlow Documentation</a></li>
</ul>
]]></description>
      <pubDate></pubDate>
      <guid>/posts/cats_vs_dogs/</guid>
    </item>
    
    <item>
      <title>My First Post</title>
      <link>/posts/my-first-post/</link>
      <description><![CDATA[<h1>My First Blog Post: Excitement and Vision</h1>
<p>Hello everyone!</p>
<p>I am thrilled to share my first blog post on this website. As a passionate Software Developer and AI Specialist, I have always been fascinated by the endless possibilities that technology offers. This website is a culmination of my experiences, skills, and aspirations, and I am excited to embark on this new journey of sharing my knowledge and ideas with you all.</p>
<h2>Why I Created This Website</h2>
<p>Over the years, I have had the privilege of working on various projects, from developing custom software solutions to founding a startup. My journey has been filled with learning, innovation, and collaboration. This website is a platform where I can document my experiences, share insights, and connect with like-minded individuals.</p>
<h2>My Vision</h2>
<p>I envision this website as a hub for tech enthusiasts, developers, and anyone interested in AI, cybersecurity, and web development. Here, I will be sharing:</p>
<ul>
<li><strong>Technical Tutorials</strong>: Step-by-step guides on programming, AI models, and web development.</li>
<li><strong>Project Highlights</strong>: Detailed breakdowns of my major project.</li>
<li><strong>Industry Insights</strong>: My thoughts on the latest trends and advancements in technology.</li>
<li><strong>Personal Experiences</strong>: Stories from my journey as a freelancer, startup founder, and continuous learner.</li>
</ul>
<h2>My Journey So Far</h2>
<p>As highlighted in my <a href="https://udaysinh.netlify.app/resume/">resume</a>, my journey has been diverse and enriching. From creating custom software solutions to managing a team at Citta Hub, I have gained valuable experience that I am eager to share. My educational background in Computer Science Engineering and continuous learning have equipped me with a versatile skill set that I am excited to leverage in this blog.</p>
<h2>Join Me on This Journey</h2>
<p>I invite you all to join me on this exciting journey. Whether you are a seasoned developer or just starting, I hope to provide valuable content that inspires and educates. Feel free to reach out, share your thoughts, and engage with the content. Together, let's explore the fascinating world of technology and innovation.</p>
<p>Thank you for visiting, and I look forward to sharing more with you soon!</p>
<p>Best regards,
Udaysinh Sapate</p>
]]></description>
      <pubDate></pubDate>
      <guid>/posts/my-first-post/</guid>
    </item>
    
  </channel>
</rss>